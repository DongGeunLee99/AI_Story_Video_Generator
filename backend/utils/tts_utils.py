"""TTS ê´€ë ¨ ìœ í‹¸ë¦¬í‹°"""

import json
import os
import re
from typing import Any, Dict, List, Tuple

from moviepy.audio.AudioClip import concatenate_audioclips
from moviepy.editor import AudioFileClip
from google.cloud import texttospeech_v1beta1 as texttospeech

from utils.auth import get_tts_client

# ì„¤ì • ìƒìˆ˜
VOICE_NAME = "ko-KR-Wavenet-C"
MAX_SUBTITLE_CHARS = 24
FPS = 24


def split_sentences(text: str) -> List[str]:
    """
    ì´ë¯¸ normalize_text()ë¥¼ ê±°ì¹œ í…ìŠ¤íŠ¸ë¥¼
    ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•œë‹¤.

    - í•œêµ­ì–´ / ì˜ì–´ ê³µí†µ
    - ë§ˆì¹¨í‘œ(.), ëŠë‚Œí‘œ(!), ë¬¼ìŒí‘œ(?) ê¸°ì¤€
    - ë”°ì˜´í‘œ(" ") í¬í•¨ ì²˜ë¦¬
    """
    pattern = r'([\.!?][""]?)'
    parts = re.split(pattern, text)

    sentences: List[str] = []

    for i in range(0, len(parts) - 1, 2):
        sentence = (parts[i] + parts[i + 1]).strip()
        if sentence:
            sentences.append(sentence)

    # ëì— êµ¬ë‘ì  ì—†ëŠ” ë§ˆì§€ë§‰ ë¬¸ì¥ ì²˜ë¦¬
    if len(parts) % 2 == 1:
        tail = parts[-1].strip()
        if tail:
            sentences.append(tail)

    return sentences


def chunk_text_by_bytes(text: str, max_bytes: int = 3000) -> List[str]:
    """UTF-8 ë°”ì´íŠ¸ ê¸¸ì´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ì—¬ëŸ¬ ì²­í¬ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤."""
    sentences = split_sentences(text)
    chunks: List[str] = []
    current = ""

    for sentence in sentences:
        candidate = (current + " " + sentence).strip()
        if len(candidate.encode("utf-8")) > max_bytes and current:
            chunks.append(current)
            current = sentence
        else:
            current = candidate

    if current:
        chunks.append(current)
    return chunks


def quantize_time(sec: float, fps: int = FPS) -> float:
    """FPS ë‹¨ìœ„ë¡œ ì‹œê°„ì„ ì–‘ìí™”í•©ë‹ˆë‹¤."""
    return round(sec * fps) / fps


def split_segment_by_length(seg: Dict[str, Any], max_chars: int) -> List[Dict[str, Any]]:
    """ê¸¸ì´ê°€ ë„ˆë¬´ ê¸´ í•œ ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ì—¬ëŸ¬ ì¤„ë¡œ ë¶„í• í•©ë‹ˆë‹¤."""
    text, start, end = seg["text"], seg["start"], seg["end"]
    duration = max(end - start, 0.1)

    if len(text) <= max_chars:
        return [seg]

    tokens = text.split()
    chunks: List[str] = []
    current = ""
    for token in tokens:
        candidate = (current + " " + token).strip()
        if current and len(candidate) > max_chars:
            chunks.append(current)
            current = token
        else:
            current = token if not current else candidate

    if current:
        chunks.append(current)

    if len(chunks) == 1 and len(chunks[0]) > max_chars:
        large_text = chunks[0]
        chunks = [
            large_text[index : index + max_chars]
            for index in range(0, len(large_text), max_chars)
        ]

    total_chars = sum(len(chunk) for chunk in chunks)
    results: List[Dict[str, Any]] = []
    current_time = start

    for index, chunk_text in enumerate(chunks):
        if index == len(chunks) - 1:
            next_start, next_end = current_time, end
        else:
            ratio = len(chunk_text) / total_chars
            delta = duration * ratio
            next_start, next_end = current_time, current_time + delta

        results.append(
            {
                "text": chunk_text,
                "start": quantize_time(next_start),
                "end": quantize_time(next_end),
            }
        )
        current_time = next_end

    return results


def synthesize_chunk(
    client,
    chunk_text: str,
    chunk_index: int,
    offset: float,
    tts_audio_dir: str,
    voice_name: str,
    speaking_rate: float,
) -> Tuple[float, List[Dict[str, Any]]]:
    """SSML `<mark>`ë¥¼ ì‚¬ìš©í•´ ì²­í¬ ë‹¨ìœ„ TTSë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    sentences = split_sentences(chunk_text)
    if not sentences:
        return 0.0, []

    ssml_parts = ["<speak>"]
    marks: List[Tuple[str, str]] = []

    for index, sentence in enumerate(sentences):
        name = f"c{chunk_index}_s{index}"
        marks.append((name, sentence))
        ssml_parts.append(
            f"<mark name='{name}'/>{sentence}<break time='0.8s'/>"
        )

    ssml_parts.append("</speak>")
    ssml_str = "".join(ssml_parts)

    request = texttospeech.SynthesizeSpeechRequest(
        input=texttospeech.SynthesisInput(ssml=ssml_str),
        voice=texttospeech.VoiceSelectionParams(
            language_code="ko-KR",
            name=voice_name,
            ssml_gender=texttospeech.SsmlVoiceGender.MALE,
        ),
        audio_config=texttospeech.AudioConfig(
            audio_encoding=texttospeech.AudioEncoding.MP3,
            speaking_rate=speaking_rate,
        ),
        enable_time_pointing=[
            texttospeech.SynthesizeSpeechRequest.TimepointType.SSML_MARK
        ],
    )

    try:
        response = client.synthesize_speech(request=request)
    except Exception as exc:
        print(f"âŒ TTS ì‹¤íŒ¨: {exc}")
        return 0.0, []

    os.makedirs(tts_audio_dir, exist_ok=True)
    out_path = os.path.join(tts_audio_dir, f"chunk_{chunk_index}.mp3")

    with open(out_path, "wb") as file:
        file.write(response.audio_content)

    clip = AudioFileClip(out_path)
    duration = clip.duration
    clip.close()

    time_map = {tp.mark_name: float(tp.time_seconds) for tp in response.timepoints}

    segments: List[Dict[str, Any]] = []
    for name, sent_text in marks:
        if name not in time_map:
            continue
        start = quantize_time(offset + time_map[name])
        segments.append({"text": sent_text.strip(), "start": start})

    segments.sort(key=lambda item: item["start"])

    for index, segment in enumerate(segments):
        if index < len(segments) - 1:
            segment["end"] = segments[index + 1]["start"]
        else:
            segment["end"] = quantize_time(offset + duration)

    return duration, segments


def generate_tts_and_subtitle(
    input_text: str,
    tts_audio_dir: str,
    tts_output_path: str,
    subtitle_json_path: str,
    google_key_file: str = None,
    voice_name: str = None,
    speaking_rate: float = 1.0,
) -> None:
    """
    ì…ë ¥ í…ìŠ¤íŠ¸ë¡œë¶€í„° TTS ì˜¤ë””ì˜¤ì™€ ìë§‰ JSON íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    ì£¼ì˜: GCP ì¸ì¦ì€ ì´ë¯¸ ì„¤ì •ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
    google_key_file íŒŒë¼ë¯¸í„°ëŠ” í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€ë˜ì§€ë§Œ ì‚¬ìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    """
    print(f"ğŸ¤ speaking_rate applied = {speaking_rate}")
    chunks = chunk_text_by_bytes(input_text)
    client = get_tts_client()

    os.makedirs(tts_audio_dir, exist_ok=True)
    os.makedirs(os.path.dirname(tts_output_path), exist_ok=True)

    all_segments: List[Dict[str, Any]] = []
    audio_paths: List[str] = []
    offset = 0.0

    selected_voice = voice_name or VOICE_NAME

    for index, chunk in enumerate(chunks):
        duration, segments = synthesize_chunk(
            client,
            chunk,
            index,
            offset,
            tts_audio_dir,
            selected_voice,
            speaking_rate,
        )
        audio_paths.append(os.path.join(tts_audio_dir, f"chunk_{index}.mp3"))
        all_segments.extend(segments)
        offset += duration

    refined: List[Dict[str, Any]] = []
    for segment in all_segments:
        refined.extend(split_segment_by_length(segment, MAX_SUBTITLE_CHARS))

    clean: List[Dict[str, Any]] = []
    trash = {'"', """, """, "'", "''", ".", "..", "...", "...."}
    for segment in refined:
        text = segment["text"].strip()
        if not text or text in trash:
            continue
        clean.append(segment)

    clips: List[AudioFileClip] = []
    for path in audio_paths:
        if not os.path.exists(path):
            print(f"âš ï¸ ì˜¤ë””ì˜¤ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•Šì•„ ê±´ë„ˆëœë‹ˆë‹¤: {path}")
            continue
        clips.append(AudioFileClip(path))

    if not clips:
        print("âŒ ë³‘í•©í•  ì˜¤ë””ì˜¤ í´ë¦½ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    final_audio = concatenate_audioclips(clips)
    final_audio.write_audiofile(tts_output_path)

    os.makedirs(os.path.dirname(subtitle_json_path), exist_ok=True)
    with open(subtitle_json_path, "w", encoding="utf-8") as file:
        json.dump(clean, file, ensure_ascii=False, indent=4)

    print(
        f"ğŸ‰ ì™„ë£Œ! ìë§‰ {len(clean)}ê°œ ìƒì„±, TTS ì €ì¥ë¨ â†’ {tts_output_path}"
    )

